# finger-tracker 開発報告書

## 1. プロジェクト概要

### 1.1 目的

本プロジェクトは、Intel RealSense D435i 深度カメラと YOLOv8-nano 物体検出モデルを組み合わせ、人間の親指と人差指に装着した赤・青のシリコン製指サック（カラーマーカー）を検出し、3D空間上の指間距離をリアルタイムに計測するシステムを開発することを目的とする。

将来的には、計測した指間距離をロボットハンドの遠隔操作指令値として使用し、バイラテラルテレオペレーションシステムへの発展を目指している。

### 1.2 背景

既存研究の柔軟物体把持制御をベースに、人間の指の開閉量をリアルタイムで高精度に計測する必要がある。RealSense D435i は RGB 画像と深度画像を同時に取得できるため、2D画像上での物体検出と3D空間上での距離計測を一つのデバイスで実現できる。

### 1.3 システム全体像

```
┌─────────────────────────────────────────────────────────────────┐
│                    finger-tracker システム                       │
│                                                                 │
│  ┌─────────┐    ┌──────────┐    ┌───────────┐    ┌──────────┐  │
│  │ capture  │ →  │ Roboflow │ →  │ training  │ →  │detection │  │
│  │ (データ  │    │ (外部    │    │ (モデル   │    │(リアル   │  │
│  │  収集)   │    │  ラベリ  │    │  学習)    │    │ タイム   │  │
│  │          │    │  ング)   │    │           │    │ 計測)    │  │
│  └─────────┘    └──────────┘    └───────────┘    └──────────┘  │
│       ↑                              ↓                ↑        │
│  RealSense D435i              models/best.pt     RealSense     │
│                                                   D435i        │
└─────────────────────────────────────────────────────────────────┘
```

---

## 2. 使用技術

### 2.1 ハードウェア

| 機器 | 仕様 | 用途 |
|------|------|------|
| Intel RealSense D435i | RGB 1280x720@30fps + 深度 1280x720@30fps | RGB画像・深度データの同時取得 |
| シリコン指サック（赤） | 親指に装着 | カラーマーカー（`red_finger` クラス） |
| シリコン指サック（青） | 人差指に装着 | カラーマーカー（`blue_finger` クラス） |

RealSense D435i はステレオ IR カメラによる深度計測方式を採用しており、各ピクセルに対して深度値（カメラからの距離）を取得できる。USB 3.0 接続が必要であり、USB 2.0 では帯域不足により 1280x720 解像度でのストリーミングができない。

### 2.2 ソフトウェア

| 技術 | バージョン | 用途 |
|------|-----------|------|
| Python | 3.12.3 | メイン言語 |
| ultralytics | >= 8.0.0 | YOLOv8-nano モデルの学習・推論 |
| pyrealsense2 | >= 2.50.0 | RealSense D435i の制御・データ取得 |
| OpenCV (opencv-python) | >= 4.8.0 | 画像処理・HSVフィルタ・描画・表示 |
| NumPy | >= 1.26.0 | 3D座標計算・カルマンフィルタ・行列演算 |
| PyYAML | >= 6.0 | 設定ファイル読み込み |
| Roboflow | Web サービス | 学習データのアノテーション（外部ツール） |

### 2.3 技術選定の理由

**Python を選択した理由**: pyrealsense2、ultralytics、OpenCV のすべてが Python で成熟したAPIを提供しており、プロトタイピングの速度を最優先した。リアルタイム処理は30fpsで十分であり、C++ による高速化は不要と判断した。

**YOLOv8-nano を選択した理由**: 3.2M パラメータで推論時間 10-20ms（CPU）と軽量であり、30fps のフレームレート要件を満たす。2クラス（赤・青指サック）の単純な検出タスクに対してはナノサイズで十分な精度が得られる。HSV 色検出単独と比較すると、照明変化や背景色との混同に強い。

**RealSense D435i を選択した理由**: 既存資産として利用可能であり、RGB と深度の同時取得・アラインメント機能を内蔵している。`rs2_deproject_pixel_to_point` 関数により、2Dピクセル座標と深度値から3D空間座標への変換をキャリブレーション不要で行える。

---

## 3. 開発プロセス

### 3.1 設計フェーズ

開発に先立ち、9本の ADR（Architecture Decision Record）を作成し、主要な技術判断を文書化した。

| ADR | テーマ | 主要な判断 |
|-----|--------|-----------|
| ADR 001 | 技術スタック選定 | Python + YOLOv8-nano + RealSense + OpenCV |
| ADR 002 | 深度計測ロジック | BB内HSVフィルタ + 3Dカルマンフィルタ + 深度フォールバック |
| ADR 003 | データパイプライン | キャプチャ→Roboflow→学習→推論の段階的フロー |
| ADR 004 | 学習戦略 | YOLOv8n、mAP50>=0.90、Roboflow拡張、100エポック |
| ADR 005 | 表示設計 | OpenCV シングルウィンドウ + オーバーレイ |
| ADR 006 | 設定管理 | YAML ファイルによる一元管理 |
| ADR 007 | ログ・記録方式 | Python logging + CSV 計測記録 |
| ADR 008 | エラーハンドリング | 起動時チェック + リトライ + グレースフルシャットダウン |
| ADR 009 | ディレクトリ構成 | src layout + git管理外ディレクトリの分離 |

### 3.2 実装フェーズ

3つのフェーズに分けて段階的に実装を進めた。

#### Phase 1: 基盤構築 + データ収集

- プロジェクト構造の構築（src layout、pyproject.toml、requirements.txt）
- `config` モジュール: YAML 設定ファイルの読み込みとデフォルト値マージ
- `capture` モジュール: RealSense からの RGB + 深度フレーム取得・保存
- 実機での動作確認（RealSense D435i 接続環境）

#### Phase 2: モデル学習

- Roboflow での学習データアノテーション（赤・青指サックのバウンディングボックス）
- `training` モジュール: YOLOv8-nano の fine-tuning 実装
- 学習実行: **mAP50 = 0.964** を達成（目標 0.90 を大幅に超過）
- 学習済みモデル `best.pt` の配置

#### Phase 3: リアルタイム計測

- `detection` モジュール: 推論 → HSVフィルタ → 深度取得 → 3D変換 → カルマンフィルタ → 距離計算 → 表示 → CSV記録
- 実機テスト・バグ修正（距離線端点の修正、深度取得順序の最適化）
- 動作確認・計測精度の検証

### 3.3 品質管理

各実装フェーズの完了後にコードレビューを実施し、以下のサイクルで品質を担保した。

```
実装 → レビュー → 指摘の分類（Critical/Medium/Low） → 修正 → 再レビュー → コミット
```

主なレビュー指摘と対応:
- config.yaml 不在時のデフォルトフォールバック廃止（ADR 008 準拠のエラー通知に変更）
- カルマンフィルタの predict() が YOLO 未検出時に呼ばれない問題の修正
- タイムスタンプの JST 統一
- 深度取得フォールバック順序の最適化（マスク内中央値を最優先に変更）

---

## 4. detection モジュールの理論詳説

detection モジュールはこのシステムの核心であり、複数の信号処理・コンピュータビジョン技術を統合している。以下では各段階の理論的背景を詳述する。

### 4.1 全体パイプライン

1フレームあたりの処理パイプラインは以下の通りである。

```
RealSense D435i
    │
    ├── RGB フレーム (1280x720, BGR8)
    │       │
    │       ▼
    │   YOLOv8-nano 推論
    │       │
    │       ▼
    │   各検出 BB に対して:
    │       │
    │       ├── BB 内 ROI 切り出し
    │       │
    │       ├── HSV 色空間フィルタ → 二値マスク生成
    │       │
    │       ├── マスク重心（モーメント法）→ ピクセル座標 (cx, cy)
    │       │
    │       └── 深度フォールバックチェーン → 深度値 d [meters]
    │                   │
    │                   ▼
    │           rs2_deproject_pixel_to_point(intrinsics, [cx, cy], d)
    │                   │
    │                   ▼
    │           3D 座標 [X, Y, Z] (meters)
    │                   │
    │                   ▼
    │           カルマンフィルタ update(measurement)
    │                   │
    │                   ▼
    │           フィルタ済み位置 [X', Y', Z']
    │
    └── 深度フレーム (1280x720, Z16) ──→ 上記の深度取得に使用

    フィルタ済み red_finger 位置 と blue_finger 位置
            │
            ▼
    ユークリッド距離 = √((X'r - X'b)² + (Y'r - Y'b)² + (Z'r - Z'b)²)
            │
            ▼
    距離値 [mm] → OpenCV 表示 + CSV 記録
```

### 4.2 YOLOv8-nano による物体検出

#### 4.2.1 YOLOv8 の概要

YOLO（You Only Look Once）は、画像全体を一度の順伝播で処理し、物体の位置（バウンディングボックス）とクラスを同時に推定するリアルタイム物体検出アーキテクチャである。

YOLOv8-nano は Ultralytics が開発した YOLO ファミリーの最軽量バリアントであり、以下の特徴を持つ。

| 項目 | 値 |
|------|-----|
| パラメータ数 | 3.2M |
| 推論時間（CPU） | 10-20ms |
| 入力サイズ | 640x640 |
| 出力 | クラスID + 信頼度 + BB座標 (x1, y1, x2, y2) |

#### 4.2.2 Fine-tuning

事前学習済みの YOLOv8n モデル（COCO データセットで学習済み）を、カスタムデータセット（赤・青指サック画像）で追加学習する。これにより、汎用的な物体検出能力を保持しつつ、指サック特有の特徴を学習できる。

本プロジェクトでは Roboflow で約300枚の画像をアノテーションし、以下の設定で学習を行った。

```yaml
epochs: 100          # 最大エポック数
patience: 50         # Early stopping の忍耐値
batch: 16            # バッチサイズ
imgsz: 640           # 入力画像サイズ
augment: false       # Roboflow 側で拡張済みのため無効化
```

データ分割は Roboflow のデフォルト（Train 70% / Validation 20% / Test 10%）を使用した。

**結果**: mAP50（IoU >= 0.5 における平均適合率）= **0.964** を達成。目標値 0.90 を大幅に上回った。

#### 4.2.3 推論の実行

毎フレーム、RGB 画像を YOLOv8 モデルに入力し、信頼度閾値（デフォルト 0.5）以上の検出結果を取得する。

```python
results = model(color_image, conf=confidence, verbose=False)
```

各検出結果には以下の情報が含まれる:
- BB 座標: `(x1, y1, x2, y2)` — 矩形領域の左上・右下ピクセル座標
- クラス ID: `red_finger` (0) または `blue_finger` (1)
- 信頼度: 0.0 〜 1.0 の確率値

### 4.3 BB 内 HSV 色フィルタによるマスク重心算出

#### 4.3.1 なぜ BB 中心を使わないのか

YOLO のバウンディングボックスは対象物体を囲む最小矩形であり、その内部には背景ピクセルが混在する。BB の幾何学的中心が必ずしも指サックの表面上にあるとは限らない。特に、指サックは小さな対象物であり、BB のアスペクト比や位置のわずかな変動が深度値の取得精度に直結する。

そこで、YOLO の BB をまず「おおまかな検出領域」として使用し、その内部で HSV 色フィルタを適用することで、指サック本体のピクセルのみを抽出する。

#### 4.3.2 HSV 色空間

RGB 色空間は照明条件の変化に敏感であるのに対し、HSV（Hue-Saturation-Value）色空間は色相・彩度・明度を分離するため、照明変化に対してよりロバストである。

```
H (Hue):        色相 — 色の種類（0-180, OpenCV の場合）
S (Saturation):  彩度 — 色の鮮やかさ（0-255）
V (Value):       明度 — 明るさ（0-255）
```

指サックは鮮やかな赤・青の色を持つため、HSV 空間での色範囲指定が有効に機能する。

#### 4.3.3 赤色の特殊処理

HSV の色相は円環構造をとり、赤色は H=0 付近と H=180 付近に分裂する。そのため、赤色の検出には2つの範囲を OR で結合する必要がある。

```yaml
red:
  lower:  [0,   120, 70]     # H=0-10 の赤
  upper:  [10,  255, 255]
  lower2: [170, 120, 70]     # H=170-180 の赤
  upper2: [180, 255, 255]
```

実装では `cv2.inRange` で各範囲のマスクを生成し、`cv2.bitwise_or` で結合する。

```python
mask = cv2.inRange(hsv, lower1, upper1)
mask2 = cv2.inRange(hsv, lower2, upper2)
mask = cv2.bitwise_or(mask, mask2)
```

青色は H=100-130 の単一範囲で十分である。

#### 4.3.4 マスク重心の算出（画像モーメント法）

二値マスクの重心は、OpenCV の `cv2.moments()` を用いて画像モーメントから算出する。

画像モーメントは以下のように定義される:

```
M_pq = Σ_x Σ_y (x^p * y^q * I(x, y))
```

ここで `I(x, y)` はピクセル `(x, y)` の値（二値マスクでは 0 または 1）。

重心座標は 0 次モーメント `M_00`（面積）と 1 次モーメント `M_10`, `M_01` から算出される:

```
cx = M_10 / M_00
cy = M_01 / M_00
```

`M_00 = 0` の場合はマスク内に有効ピクセルが存在しないことを意味し、重心は算出不能とする。

```python
M = cv2.moments(mask)
if M["m00"] == 0:
    return None
cx = int(M["m10"] / M["m00"])
cy = int(M["m01"] / M["m00"])
```

算出された `(cx, cy)` は BB 内のローカル座標であるため、BB のオフセット `(x1, y1)` を加算してフレーム全体のピクセル座標に変換する。

### 4.4 深度取得とフォールバックチェーン

#### 4.4.1 RealSense の深度データ

RealSense D435i はステレオ IR カメラ方式で深度を計測する。各ピクセルに対して `depth_frame.get_distance(x, y)` でメートル単位の深度値を取得できる。ただし、以下の理由で深度値が 0（無効）になることがある:

- 反射率の高い表面（鏡面反射）
- 深度カメラの死角（ステレオマッチング失敗）
- 近距離すぎる対象（最小計測距離以下）
- IR パターンが投影されない領域

#### 4.4.2 フォールバックチェーンの設計

深度値の欠損に対処するため、4段階のフォールバックチェーンを実装した。

```
Step 1: マスク内有効ピクセルの中央値
    │
    │ 有効ピクセルなし
    ▼
Step 2: マスク重心 (cx, cy) の深度値
    │
    │ 深度値 = 0（無効）
    ▼
Step 3: BB 内全体の有効ピクセル探索（2ピクセル間隔サンプリング）
    │
    │ 有効ピクセルなし
    ▼
Step 4: 直前の有効深度値を保持（タイムアウト 0.5 秒）
    │
    │ タイムアウト超過
    ▼
計測不能（深度値 = 0）
```

**Step 1 を最優先にした理由**: 1ピクセルの深度値はノイズの影響を受けやすい。マスク内の複数ピクセルから中央値を取ることで、外れ値の影響を抑え、より安定した深度推定が得られる。中央値（median）は平均値と異なり、少数の異常値に対してロバストである。

**Step 4 のタイムアウト**: 直前値を無期限に保持すると、指がフレームアウトした後も古い位置を使い続けてしまう。0.5秒のタイムアウトにより、約15フレーム分の猶予を与えつつ、過度に古い値の使用を防ぐ。

#### 4.4.3 深度精度の特性

RealSense D435i の深度精度は距離の2乗に比例してノイズが増大する。

```
深度ノイズ σ_z ∝ z²
```

これは、ステレオ視差 `d` と深度 `z` の関係 `z = f * B / d`（f: 焦点距離、B: 基線長）から導かれる。視差の量子化誤差 `Δd` が一定の場合、深度誤差 `Δz` は以下のように近似される:

```
Δz ≈ z² * Δd / (f * B)
```

したがって、遠距離ほど深度の不確かさが増大する。この特性はハードウェアに起因するため、ソフトウェアで完全に補正することはできないが、マスク内中央値の使用とカルマンフィルタによる時間方向の平滑化で緩和している。

### 4.5 2D ピクセル → 3D 空間座標変換

#### 4.5.1 ピンホールカメラモデル

RealSense D435i のカメラ内部パラメータ（イントリンシクス）は、ピンホールカメラモデルに基づく。3D 空間上の点 `(X, Y, Z)` は、以下の式でピクセル座標 `(u, v)` に投影される:

```
u = fx * X / Z + cx
v = fy * Y / Z + cy
```

ここで:
- `fx`, `fy`: 焦点距離（ピクセル単位）
- `cx`, `cy`: 光学中心（主点、ピクセル単位）

#### 4.5.2 逆投影（Deprojection）

ピクセル座標 `(u, v)` と深度値 `Z` から 3D 座標 `(X, Y, Z)` を復元するのが逆投影である。上記の式を逆に解くと:

```
X = (u - cx) * Z / fx
Y = (v - cy) * Z / fy
Z = Z（深度値そのまま）
```

pyrealsense2 の `rs2_deproject_pixel_to_point` 関数は、この逆投影をレンズ歪み補正を含めて行う。

```python
point_3d = rs.rs2_deproject_pixel_to_point(intrinsics, [cx, cy], depth)
# point_3d = [X, Y, Z]  (meters)
```

`intrinsics` はカメラの起動時に `get_intrinsics()` で取得した内部パラメータであり、焦点距離・主点・歪み係数を含む。

#### 4.5.3 深度フレームのアラインメント

RealSense D435i の RGB カメラと深度カメラは物理的に異なる位置に設置されている。そのため、RGB 画像上のピクセル座標 `(u, v)` の深度値を正しく取得するには、深度フレームを RGB フレームの視点にアラインする必要がある。

```python
align = rs.align(rs.stream.color)
aligned = align.process(frames)
```

アラインメント後は、RGB 画像上の任意のピクセルに対して `depth_frame.get_distance(u, v)` で正しい深度値が得られる。

### 4.6 カルマンフィルタによる時間方向ノイズ除去

#### 4.6.1 カルマンフィルタの概要

カルマンフィルタは、ノイズを含む観測データから、システムの内部状態を最適に推定するための再帰的アルゴリズムである。1960年に Rudolf E. Kalman によって提案された。

カルマンフィルタの本質は、「運動モデルによる予測」と「センサ観測」を、それぞれの不確かさ（共分散）に基づく最適な重みで統合することにある。

本システムでは、各指サックの 3D 位置を独立にカルマンフィルタで追跡する。

#### 4.6.2 状態空間モデル

**状態ベクトル** `x` (6次元):

```
x = [x, y, z, vx, vy, vz]^T
```

- `x, y, z`: 3D空間上の位置 (meters)
- `vx, vy, vz`: 各軸方向の速度 (meters/sec)

**観測ベクトル** `z_obs` (3次元):

```
z_obs = [x_obs, y_obs, z_obs]^T
```

`rs2_deproject_pixel_to_point` の出力がそのまま観測値となる。

#### 4.6.3 等速度モデル（運動方程式）

フレーム間隔 `dt` (= 1/30 秒 ≈ 33ms) における等速度運動を仮定する:

```
x(t+1) = x(t) + vx(t) * dt
y(t+1) = y(t) + vy(t) * dt
z(t+1) = z(t) + vz(t) * dt
vx(t+1) = vx(t)
vy(t+1) = vy(t)
vz(t+1) = vz(t)
```

これを行列形式で表すと:

```
x(t+1) = F * x(t) + w(t)
```

**状態遷移行列 F** (6x6):

```
F = | 1  0  0  dt  0   0  |
    | 0  1  0  0   dt  0  |
    | 0  0  1  0   0   dt |
    | 0  0  0  1   0   0  |
    | 0  0  0  0   1   0  |
    | 0  0  0  0   0   1  |
```

`w(t)` はプロセスノイズであり、モデルの不完全さ（加速・減速・方向転換）を確率的に表現する。

**観測行列 H** (3x6):

```
H = | 1  0  0  0  0  0 |
    | 0  1  0  0  0  0 |
    | 0  0  1  0  0  0 |
```

位置成分のみを観測する。速度は直接観測できず、位置の時間変化から間接的に推定される。

#### 4.6.4 ノイズモデル

**プロセスノイズ共分散 Q** (6x6):

```
Q = q * I_6
```

`q = 0.01` は対角成分に設定される。この値はモデルの不確かさの大きさを表す。`q` が大きいほど「モデルの予測を信頼しない」（= 観測に追従しやすくなるが、ノイズにも敏感）、小さいほど「モデルの予測を信頼する」（= 滑らかだが追従が遅れる）。

**観測ノイズ共分散 R** (3x3):

```
R = r * I_3
```

`r = 0.1` は RealSense の深度ノイズの大きさを表す。`r` が大きいほど「観測を信頼しない」（= 予測に依存、滑らか）、小さいほど「観測を信頼する」（= 追従が速いがノイズが乗る）。

`Q` と `R` の比率がフィルタの応答特性を決定する。`Q/R = 0.01/0.1 = 0.1` は、観測よりもモデル予測をやや重視する設定であり、深度ノイズの平滑化を優先している。

#### 4.6.5 予測ステップ（Predict）

毎フレーム、全てのフィルタに対して無条件に予測ステップを実行する。

```python
def predict(self):
    self.x = self.F @ self.x          # 状態予測: x̂(t|t-1) = F * x̂(t-1|t-1)
    self.P = self.F @ self.P @ self.F.T + self.Q  # 共分散予測: P(t|t-1) = F * P(t-1|t-1) * F^T + Q
```

1. **状態予測**: 等速度モデルに基づき、現在の位置と速度から次の時刻の位置を予測する
2. **共分散予測**: 推定の不確かさがプロセスノイズ `Q` の分だけ増大する

YOLO が検出に失敗したフレームでも予測ステップは実行される。これにより、一時的な未検出でも追跡を継続できる（予測位置は等速度運動に従って外挿される）。

#### 4.6.6 更新ステップ（Update）

YOLO が検出に成功し、有効な 3D 座標が得られた場合にのみ更新ステップを実行する。

```python
def update(self, measurement):
    y = measurement - self.H @ self.x           # イノベーション（残差）
    S = self.H @ self.P @ self.H.T + self.R     # イノベーション共分散
    K = self.P @ self.H.T @ np.linalg.inv(S)    # カルマンゲイン
    self.x = self.x + K @ y                     # 状態更新
    self.P = (np.eye(6) - K @ self.H) @ self.P  # 共分散更新
```

各ステップの意味:

1. **イノベーション `y`**: 実際の観測値と予測観測値の差。「予想外の観測」の度合いを表す
2. **イノベーション共分散 `S`**: イノベーションの不確かさ。予測の不確かさと観測ノイズの和
3. **カルマンゲイン `K`**: 予測と観測をどの比率で混合するかを決定する重み行列
   - `K` が大きい → 観測を重視（予測の不確かさが大きい時）
   - `K` が小さい → 予測を重視（観測ノイズが大きい時）
4. **状態更新**: 予測状態をイノベーションの方向に `K` の分だけ修正する
5. **共分散更新**: 観測を取り込んだことで不確かさが減少する

#### 4.6.7 初期化

最初の観測時は、フィルタの状態を直接観測値で初期化する（速度は 0 と仮定）。

```python
if not self._initialized:
    self.x[:3] = measurement  # 位置 = 最初の観測値
    self._initialized = True
    return
```

これにより、初期の共分散が大きい状態でも即座に正しい位置にロックオンできる。

#### 4.6.8 各指独立追跡の利点

赤指サックと青指サックにそれぞれ独立したカルマンフィルタを割り当てている。

```python
kf_red  = KalmanFilter3D(q=0.01, r=0.1, dt=1/30)
kf_blue = KalmanFilter3D(q=0.01, r=0.1, dt=1/30)
```

独立追跡の利点:
- 片方の指サックが一時的に遮蔽されても、もう片方の追跡は影響を受けない
- 各指の運動は独立しているため、結合モデルよりもシンプルで安定
- 結合 12D フィルタ（両指の状態を1つのフィルタで追跡）では、指間の相関モデルの設計が困難

### 4.7 ユークリッド距離計算

フィルタ済みの 3D 位置 `pos_red = [Xr, Yr, Zr]` と `pos_blue = [Xb, Yb, Zb]` から、ユークリッド距離を算出する:

```
distance = √((Xr - Xb)² + (Yr - Yb)² + (Zr - Zb)²)
```

単位はメートルであるため、1000 を掛けてミリメートルに変換する。

```python
distance_mm = float(np.linalg.norm(red_pos - blue_pos) * 1000)
```

この距離は 3D 空間上の直線距離であり、カメラの視線方向に依存しない。つまり、カメラに対して指を横に開いても、奥行き方向に開いても、同じ物理距離であれば同じ値が得られる。

### 4.8 表示システム

OpenCV の `imshow` + `putText` + `rectangle` + `line` による単一ウィンドウのオーバーレイ表示を採用した。

```
┌─────────────────────────────────────────────┐
│ FPS: 30  Conf: 0.95/0.93                    │  ← 上部: FPS + 検出信頼度
│                                             │
│   ┌───┐              ┌───┐                  │
│   │RED│──── 45.2mm ──│BLU│                  │  ← BB + 距離線 + 距離値
│   └───┘              └───┘                  │
│                                             │
│ Distance: 45.2 mm                           │  ← 下部: 距離の大表示
└─────────────────────────────────────────────┘
```

- **BB（バウンディングボックス）**: YOLO の検出領域を矩形で描画。赤指サックは赤色、青指サックは青色
- **距離線**: 両指サックのマスク重心間を緑色の線で結ぶ。中点に距離値をテキスト表示
- **FPS**: フレーム間隔の逆数で算出。リアルタイム性の監視に使用
- **信頼度**: YOLO の検出信頼度。赤/青それぞれの値を表示

### 4.9 CSV 記録

毎フレームの計測データを CSV ファイルに記録する。

```csv
timestamp,distance_mm,red_x,red_y,red_z,blue_x,blue_y,blue_z,red_conf,blue_conf
2026-02-26T10:12:40.009,69.2,0.0617,-0.0080,0.4848,-0.0044,-0.0283,0.4838,0.92,0.91
```

| カラム | 内容 | 単位 |
|--------|------|------|
| timestamp | ISO 8601 形式（JST、ミリ秒精度） | — |
| distance_mm | 指間距離 | mm |
| red_x, red_y, red_z | 赤指サックのフィルタ済み3D座標 | m |
| blue_x, blue_y, blue_z | 青指サックのフィルタ済み3D座標 | m |
| red_conf | 赤指サックの検出信頼度 | 0.0-1.0 |
| blue_conf | 青指サックの検出信頼度 | 0.0-1.0 |

検出できなかったフレームでは対応するカラムが空文字列となる。

### 4.10 エラーハンドリングとグレースフルシャットダウン

#### 起動時チェック
- モデルファイル `best.pt` の存在確認
- RealSense D435i の接続確認（`pipeline.start()` の例外捕捉）

#### 実行中の耐障害性
- フレーム取得失敗: 最大3回リトライ（1秒間隔）、復帰不能なら終了
- 深度欠損: フォールバックチェーンで対処（4.4節）
- YOLO 未検出: カルマンフィルタの予測ステップで追跡継続

#### グレースフルシャットダウン

```python
try:
    while True:
        # メインループ
except KeyboardInterrupt:
    logger.info("ユーザーによる終了")
finally:
    csv_file.flush()      # CSV バッファをフラッシュ
    csv_file.close()      # CSV ファイルを閉じる
    pipeline.stop()       # RealSense パイプラインを停止
    cv2.destroyAllWindows()  # OpenCV ウィンドウを破棄
```

`q` キー、ESC キー、Ctrl+C、予期しない例外のいずれの場合でも、`finally` ブロックにより CSV データの保存と RealSense の安全な終了が保証される。

---

## 5. 計測精度

### 5.1 モデル精度

| 指標 | 値 |
|------|-----|
| mAP50 | 0.964 |
| 目標値 | 0.90 |

### 5.2 距離計測精度

実機テストの CSV ログから分析した、深度帯別の距離計測値（指の間隔を一定に保った状態）:

| 深度帯 | 平均距離 | 標準偏差 | サンプル数 |
|--------|----------|----------|-----------|
| 0.3-0.4m | 71.5mm | 24.0mm | 354 |
| 0.4-0.5m | 83.9mm | 46.8mm | 148 |
| 0.5-0.6m | 85.7mm | 69.5mm | 73 |
| 0.6-0.7m | 79.0mm | 34.3mm | 50 |
| 0.7-0.8m | 85.9mm | 56.8mm | 38 |
| 0.8-0.9m | 92.1mm | 37.8mm | 48 |

深度が増加しても系統的な偏り（距離が一方的に増減する傾向）は見られない。標準偏差は RealSense の深度ノイズ特性（距離の2乗に比例）に起因する。

---

## 6. プロジェクト構造

```
finger-tracker/
├── CLAUDE.md                    # 開発ルール・プロジェクト概要
├── README.md                    # プロジェクト説明
├── REPORT.md                    # 本報告書
├── config.yaml                  # アプリケーション設定
├── pyproject.toml               # Python パッケージ定義
├── requirements.txt             # 依存関係
├── src/
│   └── finger_tracker/
│       ├── __init__.py
│       ├── config/              # 設定管理
│       │   ├── __init__.py      # load_config(), _deep_merge()
│       │   └── __main__.py      # 設定表示エントリポイント
│       ├── capture/             # データ収集
│       │   ├── __init__.py      # run(), _find_next_index()
│       │   └── __main__.py      # 収集エントリポイント
│       ├── training/            # モデル学習
│       │   ├── __init__.py      # train(), evaluate_and_deploy()
│       │   └── __main__.py      # 学習エントリポイント
│       └── detection/           # リアルタイム計測
│           ├── __init__.py      # KalmanFilter3D, run(), 各処理関数
│           └── __main__.py      # 計測エントリポイント
├── docs/
│   ├── archive/                 # 初期仕様書
│   ├── design/decisions/        # ADR 001-009
│   ├── plans/resolved/          # 完了した実装計画 001-003
│   ├── review/                  # レビューガイド・基準
│   └── status/                  # 実装ステータス・ロードマップ
├── data/                        # 学習データ（git管理外）
├── models/                      # 学習済みモデル（git管理外）
├── logs/                        # 計測ログ（git管理外）
└── runs/                        # ultralytics 学習出力（git管理外）
```

---

## 7. 今後の展望

- **テレオペレーション連携**: 計測した指間距離をロボットハンドの把持指令値として送信するインターフェースの実装
- **カルマンフィルタのチューニング**: Q/R パラメータの実験的最適化による追従性と安定性のバランス調整
- **複数指対応**: 3本以上の指サックを追跡し、より複雑なハンドジェスチャを計測
- **ネットワーク通信**: 計測PCからロボット制御PCへのリアルタイムデータ送信機能の追加
